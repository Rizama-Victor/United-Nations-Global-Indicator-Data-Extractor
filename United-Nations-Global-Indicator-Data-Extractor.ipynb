{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb2c469-40df-44ec-8629-d4b740643d82",
   "metadata": {},
   "source": [
    "## Web Scraping Project: Global Indicators Data Extraction\n",
    "\n",
    "### Project Overview\n",
    "The goal of this project was to scrape data from the [United Nations Database - Global Indicators](https://data.un.org/default.aspx) to gather comprehensive global indicator data for every country in the world, as well as various regions. The project focused on four key indicators:\n",
    "- **Economic Indicators**\n",
    "- **Environmental and Infrastructural Indicators**\n",
    "- **General Information**\n",
    "- **Social Indicators**\n",
    "\n",
    "### Data Collection Process\n",
    "For each country and region, the web scraping tool was programmed to navigate the respective web pages of the United Nations Database and extract data for each of the four indicators. The process was repeated for all countries and the following regions:\n",
    "- **Global** (The whole world)\n",
    "- **Africa** (Northern Africa, Sub-Saharan Africa, Eastern Africa, Middle Africa, Southern Africa, Western Africa)\n",
    "- **Americas** (Northern America, Latin America and Caribbean, Caribbean, Central America, South America)\n",
    "- **Asia** (Central Asia, Eastern Asia, Southern Asia, South-Eastern Asia, Western Asia)\n",
    "- **Europe** (Northern Europe, Southern Europe, Western Europe, Eastern Europe)\n",
    "- **Oceania** (Australia and New Zealand, Melanesia, Micronesia, Polynesia)\n",
    "\n",
    "### Output\n",
    "The data for each of the four indicators was saved into separate CSV files. For every country and region, a folder named after the country or region contained four CSV files, each corresponding to one of the indicators. For example, a country's folder would include:\n",
    "- `Economic_Indicators.csv`\n",
    "- `Environmental_and_Infrastructure_Indicators.csv`\n",
    "- `General_Information.csv`\n",
    "- `Social_Indicators.csv`\n",
    "\n",
    "Similarly, each region also had four CSV files to represent its respective indicators, ensuring a structured and organized dataset for future analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df15a8-bd09-4bce-9d19-f65efc5fe382",
   "metadata": {},
   "source": [
    "### Importation of Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "304e1c8b-fd97-411b-955c-1f854dc5eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- imports the Beautiful Soup Library for Parsing HTML Code.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -- imports the request library for making HTML requests to a website.\n",
    "\n",
    "import requests\n",
    "\n",
    "# -- imports the regex library for parsing of data\n",
    "\n",
    "import re\n",
    "\n",
    "# -- imports pandas library for creating and working with dataframe (s)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -- imports operating system library\n",
    "\n",
    "import os\n",
    "\n",
    "# -- imports tkinter library for incorporating GUI\n",
    "\n",
    "import tkinter as tk\n",
    "\n",
    "# -- imports the ttk library, entry and label library for creating GUI element, single line input fields and displaying text and images within GUI respectively\n",
    "\n",
    "from tkinter import ttk, Entry, Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39affc28-f639-4b9d-9006-779e3d0076a7",
   "metadata": {},
   "source": [
    "### Important things to Note\n",
    "- A successful response from a website is 200 (i.e \"Request has succeeded\"). Check: [HTTP Request Response Summary](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) for meaning of other potential responses.\n",
    "- The Python Classes in this Script use Class Inheritance to enable the use of parent class methods in a child class.\n",
    "- A majority of the class methods in this script make use of function chaining by using the output of previously defined functions as their input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b075ffd-f444-464f-aa96-9b1c8beaff16",
   "metadata": {},
   "source": [
    "### Class for Graphical User Interface (GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "971ffb6f-4402-4f39-a9cb-f1fdb69b1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gui:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"Class Initialization.\"\"\"\n",
    "        self.root = tk.Tk()  # creates the main root widget\n",
    "        self.entry = self.create_entry()  # creates and set up the entry widget\n",
    "        self.root.title(\"WebScraper\") # sets the title of the GUI window\n",
    "        self.root.iconbitmap(\"web_scraping_icon.ico\") # inserts the icon on the gui window\n",
    "        self.root.geometry(\"400x200\")  # sets the size of the root window (width x height)\n",
    "        \n",
    "\n",
    "    \n",
    "    # --- method to create an entry widget with default attributes\n",
    "    def create_entry(self):\n",
    "        \n",
    "        e = Entry(self.root, width=50, borderwidth=3, foreground = \"black\", background = \"white\") # creates the widget using the specifications provided\n",
    "        global default_txt\n",
    "        default_txt = \"Enter the link to the Website you'll like to scrape\"\n",
    "        e.insert(0, default_txt)  # Default text in entry at postion 0  (obviosuly the only position in the widget)\n",
    "        e.place(relx=0.5, rely=0.5, anchor='center')  # positions the entry widget in the middle of the window using place\n",
    "        \n",
    "        # binding events that will be performed when a specific action is initiated\n",
    "        e.bind(\"<FocusIn>\", self.clear_default_text) # Bind focus event (i.e the event to look out for) to clear the default text function (i.e the handler) when clicked on\n",
    "        e.bind(\"<Return>\", self.retrieve_link) # Bind Enter Keyboard Key event (i.e the event to Look out for) to process the input (i.e the handler) when enter key is pressed\n",
    "        return e # returns the entry widget\n",
    "        \n",
    "\n",
    "    # --- method to create progress bar widget of the data scraping\n",
    "    def create_progress_bar(self):\n",
    "        \n",
    "        progress = ttk.Progressbar(self.root, orient='horizontal', length=300, mode='determinate')\n",
    "        progress.place(relx=0.5, rely=0.55, anchor='center')  # positions the progress bar widget below the entry widget in the window using place\n",
    "        return progress # returns the progress bar\n",
    "        \n",
    "\n",
    "    # --- method to clear the default text when the user starts typing\n",
    "    def clear_default_text(self, event):\n",
    "        \n",
    "        # conditional to check for default text is present in the entry widget\n",
    "        if self.entry.get() == default_txt:\n",
    "            self.entry.delete(0, 'end')  # Clear the default text if true\n",
    "            \n",
    "\n",
    "    # --- method to retrieve the link entered by the user in the entry widget\n",
    "    def retrieve_link(self, event):\n",
    "        link = self.entry.get() # gets the link entered by the user and stores it in the \"link\" variable\n",
    "        self.processed_link = link # store the link for later use\n",
    "        self.root.destroy() # close the GUI window\n",
    "        \n",
    "\n",
    "    # --- method to return the retrieved link\n",
    "    def get_link(self):\n",
    "        \n",
    "        return getattr(self, 'processed_link', None) # returns the value of 'processed_link' attribute if it exists; otherwise, returns None.\n",
    "\n",
    "        \n",
    "\n",
    "    # --- method to run the tkinter main loop\n",
    "    def run(self):\n",
    "        \n",
    "        self.root.mainloop() # starts the Tkinter main event loop, keeping the GUI window active and responsive.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde5959-4385-47d7-8b95-5d8a2d2a4ef4",
   "metadata": {},
   "source": [
    "### Class for Accessing the Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f360bc8b-7402-44e0-97ea-6b39eac43b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebsiteAccess:\n",
    "\n",
    "    \"\"\"Class Initialization\"\"\"\n",
    "    def __init__(self):\n",
    "        self.gui = Gui() # create an instance of the GUI class\n",
    "        self.gui.run() # Run the GUI        \n",
    "\n",
    "    # url1 = \"https://data.un.org/en/reg/g1.html\"\n",
    "    # url2 = \"https://data.un.org/default.aspx\"\n",
    "    \n",
    "\n",
    "    # --- class method for taking in the URL from the user, and returning the website reponse back to the user  \n",
    "    \n",
    "    def website_response(self):\n",
    "    \n",
    "        # url = input(\"Kindly enter the url of the website you want to scrape: \") # requests website link from the user\n",
    "        url = self.gui.get_link() # retrieves the link entered by the user by calling the retrieve_link function.\n",
    "    \n",
    "        # try-except block to handle any unexpected error that might occur\n",
    "        try:\n",
    "            response = requests.get(url) # sends a request to website and gets a response, 200 means success\n",
    "            \n",
    "            response.raise_for_status() # returns a HTPPError if the response code was unsuccessful\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print( f\"Failed to get response from the website: {e}\") # prints back the error that occurs\n",
    "            \n",
    "        # returns the website's response and raw html code in an easy to read hierarchical format    \n",
    "        return response\n",
    "\n",
    "\n",
    "    # --- class method to get the selected table using its index\n",
    "    \n",
    "    def table_html_code(self, table_index, ech_webpage_soup):\n",
    "        \n",
    "        table = ech_webpage_soup.find_all(\"details\")[table_index] # gets the html code for the selected table      \n",
    "        return table # returns htmll code for current table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57fc018-d923-43b3-8f7b-1b4c949d2c07",
   "metadata": {},
   "source": [
    "### Class for Scraping the Website with Parent Class \"WebsiteAccess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4710f80d-3717-4649-b21c-bb517e4960ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WebsiteScraping(WebsiteAccess):\n",
    "    \n",
    "    \"\"\"Class Initialization\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    #  --- class method to get the name of the table\n",
    "   \n",
    "    def get_table_name(self, ech_webpage_soup, table_index, title_tag = \"summary\"):\n",
    "        \n",
    "        table_info = self.table_html_code(table_index, ech_webpage_soup) # calls the table_html_code method to get the html doc of the selected (or current) table\n",
    "        table_title = table_info.find(title_tag) # uses the \"summary\" html tag to get the title of the selected (or current) table \n",
    "        table_name = table_title.text.strip(\"\\n\") # extracts the text data (type) element from the \"summary\" tag while stripping off newline characters \n",
    "        return table_name # returns the name of the table\n",
    "\n",
    "        \n",
    "    # --- class method to get all rows in the table\n",
    "    \n",
    "    def table_data(self, table_index : int, ech_webpage_soup, header_title_tag : str = \"th\", row_data : str = \"tr\"): \n",
    "        \n",
    "        table_info = self.table_html_code(table_index, ech_webpage_soup) # calls the table_html_code method and returns table specified by index while assigning it to the \"table\" variable\n",
    "        table = table_info.find(\"table\") # uses the \"table\" html tag to get the all contents in the of the selected (or current) table\n",
    "        all_row_data = [] # creates a list object and assigns it to the variable \"all_row_data\"        \n",
    "        table_rows = table.find_all(\"tr\") # uses the \"tr\" html tag to get all rows in the table\n",
    "        \n",
    "        # loops through each row in the table and extracts the text data (type)\n",
    "        for row in table_rows:\n",
    "            each_row = row.find_all(\"td\") # uses the \"td\" html tag to get the data in each row (or each data cell)\n",
    "            each_row_data = [data.text.strip(\"\\n\") for data in each_row]  # loops through each row and extracts the text data (type) element from the \"tr\" tag while stripping off newline characters and stores them in a list using list comprehension     \n",
    "            \n",
    "            all_row_data.append(each_row_data) # appends the data in each row to the list \"all_row_data\" (i.e a list of lists)\n",
    "    \n",
    "        return all_row_data # returns all the rows in the table in a list object \"all_row_data\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c2247-09c8-42fe-95d0-298cc077df3c",
   "metadata": {},
   "source": [
    "### Class for Extracting Data from the Website with Parent Class \"WebsiteScraping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ffca70cf-73f9-401b-a7c3-9bf5ea8d402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WebsiteDataExtraction(WebsiteScraping):\n",
    "    \n",
    "    \"\"\"Class Initialization\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # --- class method to create dataframe\n",
    "    \n",
    "    def get_into_df(self, table_index : int, ech_webpage_soup):\n",
    "        \n",
    "        table_df = pd.DataFrame(self.table_data(table_index, ech_webpage_soup)) # creates a new data frame for the table using the output of the \"table_data\" class method (i.e the \"all_row_data\" list)\n",
    "        return table_df # returns the created dataframe\n",
    "        \n",
    "    \n",
    "    # --- class method to write dataframe to a csv file\n",
    "    \n",
    "    def convert_df2_csv(self, section_name, folder_name, table_index, ech_webpage_soup): \n",
    "   \n",
    "        folder_path = os.path.join(section_name, folder_name) # creates path to folder\n",
    "\n",
    "        # creates path to folder directory if it does not exist\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            \n",
    "        file_path = os.path.join(folder_path, f\"{self.get_table_name(ech_webpage_soup, table_index)}.csv\") # creates path where each csv file will be stored\n",
    "            \n",
    "        # try-except block to handle any errors that may arise\n",
    "        try:\n",
    "            csv_file = self.get_into_df(table_index, ech_webpage_soup).to_csv(file_path, index = False) # writes the created dataframe to a csv file using the \"table_name\" class method\n",
    "            return csv_file # returns the created csv file\n",
    "            \n",
    "        except PermissionError as pe:\n",
    "            print(\"Permission Error: The file you are trying to modify is currently in use. Kindly close it or use another file\") # prints this out incase a Permission Error is raised\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An Error Occured: {e}\") # prints this incase any other error is raised.\n",
    "            \n",
    "\n",
    "    # --- class method to write all available tables in the website to csv files.\n",
    "    \n",
    "    def get_all_tables(self):\n",
    "\n",
    "        website_access = WebsiteAccess() # creates an instance of the website class\n",
    "        response = website_access.website_response() # calls the website response method in the WebsiteAccess class to get the website's response and assigns the result to the \"response\" variable\n",
    "        \n",
    "        # response = self.website_response() # class method call for the website's response and assigns the result to the \"response\" variable\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') # gets the raw response and returns it in html format    \n",
    "        div_tag = soup.find_all('div', class_='Left', style='width: 32.5%;') # retrieve all <div> tags that contain links to different countries' global indicators.\n",
    "        \n",
    "        for section in range(len(div_tag)):\n",
    "            section_name = \"Section_\" + str(section + 1) # creates the name of the current section's folder\n",
    "            div_tag = soup.find_all('div', class_='Left', style='width: 32.5%;')[section] # retrieves the current <div> tag that contain links to current section of global indicators.\n",
    "            \n",
    "            li_tags = div_tag.find(\"ul\").find_all(\"li\") # uses the \"ul\" and \"li\" html tag to find the list of countries with their respetive names and hyperlinks          \n",
    "\n",
    "            for li_tag in li_tags:\n",
    "                \n",
    "                folder_name = li_tag.text # extracts the name of each subfolder in the current section\n",
    "                link_tag = li_tag.find(\"a\") # gets the content of the \"a\" html tag from within the \"li\" tag\n",
    "                hyper_link = link_tag.get(\"href\") # gets the hyper link from the link_tag (i.e a tag)\n",
    "                hyper_link = \"https://data.un.org/\" + hyper_link\n",
    "                # print(hyper_link)\n",
    "                \n",
    "                # try-except block to handle any unexpected error that might occur\n",
    "                try:\n",
    "                    ech_hyper_lnk_response = requests.get(hyper_link) # sends a request to website and gets a response, 200 means success\n",
    "                    ech_hyper_lnk_response.raise_for_status() # returns a HTPPError if the response code was unsuccessful\n",
    "                    \n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print( f\"Failed to get response from the website: {e}\") # prints back the error that occurs\n",
    "                \n",
    "                ech_webpage_soup = BeautifulSoup(ech_hyper_lnk_response.text, 'html.parser') # gets the raw response in html for each subwebpage \n",
    "    \n",
    "               # conditional to check for the availability of a table in the website\n",
    "               # NOTE: From the website's inspection, the \"details\" tag is what holds each table in the website\n",
    "                if ech_webpage_soup.find_all(\"details\"):            \n",
    "                    total_no_of_tables = len(ech_webpage_soup.find_all(\"details\")) # returns the total number of tables in the website -- works for any table\n",
    "                    \n",
    "                    # for loop to iterate through all tables extract only tables that are NOT empty\n",
    "                    for table_index in range(total_no_of_tables):\n",
    "                        \n",
    "                        # conditional statement to check if table is empty\n",
    "                        if self.get_into_df(table_index, ech_webpage_soup).empty != True:    \n",
    "                            self.convert_df2_csv(section_name, folder_name, table_index, ech_webpage_soup) # writes the current table dataframe to a csv file if table is NOT empty\n",
    "                        else:\n",
    "                            pass # omits empty tables\n",
    "                        \n",
    "                else:\n",
    "                    print(\"There's no tabular data in this website. Please provide another link.\")\n",
    "                \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb87ed8-8213-4a79-a615-bceb2b6382aa",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- The \"get_all_tables\" method initiates the whole webscraping process by getting response from the website and creating a soup object.\n",
    "- The \"get_all_tables\" method is able to obtain the data for tables in the website by only calling the \"convert_df2_csv\" function due to function chaining. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6bc0ff-75c8-4d7e-8c3f-bb50c2a9822f",
   "metadata": {},
   "source": [
    "### Creating the Website Extraction Class Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "275dfadc-4ac2-4aff-927b-4999d6c48e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates an instance of the WebsiteDataExtraction class \n",
    "extraction = WebsiteDataExtraction()\n",
    "\n",
    "# calls the get_all_tables() method\n",
    "extraction.get_all_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
